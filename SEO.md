SEO
====
Notes basés sur le site [moz.com](https://moz.com/beginners-guide-to-seo):

La hiérarchie des besoins de Mazlow est une théorie psychologique qui prioritarise les besoins humains les plus fondamentaux plutôt que les besoins les plus avancés. Les seconds besoins ne peuvent être atteints que si les premiers sont remplis. le site moz.com construit sur une hiérarchie SEO des besoins de Mazlow. 

**SEO = Search Engine Optimization** C'est une pratique pour augmenter la qualité et la quantité de la fréquentation de sites par le biais des résultats non payés des moteurs de recherche. Il s'agit de comprendre ce que les gens recherchent, le vocabulaire qu'ils utilisent, les réponses qu'ils souhaitent obtenir.

Les moteurs de recherche sont des machines à réponses. Ils obtiennent leurs réponses par un système de *crawling & indexing* qui leur permet de découvrir et cataloguer tout le contenu présent sur le web pour ensuite l'ordonner en fonction de la pertinence de la requête. Ce processus de pertinennce s'appelle le **ranking**.

Quels résultats sont considérés comme organiques ?
-------------------------------------------------

**SERPs : Search Engine Results Pages**, càd les pages de résultats. Elles sont constituées de réponses organiques (non payées) et d'annonces. 

SERP FEATURE : format de résultats dynamiques. Ex : "Les gens ont aussi recherché", les *snippets*...

Ne jamais oublier que les moteurs de recherche font leur argent grâce à la pub.

Certaines des *SERP FEATURES* sont organiques et **peuvent donc être influencées** par le SEO. Certaines *features* de recherche, bien que non payées, **ne peuvent pas être influencées** par le SEO. Ex : Wikipedia. 

Importance du SEO
----------------

Non seulement les résultats organiques couvrent une plus grande part de *terrain digital* mais, en plus, ils **apparaissent comme plus crédibles et obtiennent plus de clics**. 

Le SEO est aussi un des seuls canals numériques qui, lorsque mis en place correctement, continue d'apporter des dividendes au fur et à mesure que le temps passe. 

Optimiser un site permet de **délivrer de meilleures informations** et donc d'**obtenir une meilleure indexation** sur les moteurs de recherche.

White Hat SEO / Black Hat SEO
----------------------------

**White Hat SEO : techniques, pratiques et stratégies qui se plient aux règles des moteurs de recherche**. L'objectif est de fournir du contenu de valeur. 

**Black Hat SEO : techniques et stratégies pour spammer/tromper les moteurs de recherche.** Méthode dangereuse qui peut aboutir à une désindexation

Les Principes SEO selon les grands moteurs de recherche:
-------------------------------------------------------

**GOOGLE**

[Google webmaster guidelines](https://support.google.com/webmasters/answer/35769?hl=fr)

**Les principes de base** :

- **Construire des pages pour les utilisateurs**, pas les moteurs de recherches
- **Ne pas tromper les utilisateurs**
- **Eviter les astuces** pour améliorer le ranking
- Réfléchir à ce qui rend **son site unique**, de valeur.

**Ce qu'il faut éviter** :

- Du **contenu généré automatiquement**
- Participer à des **systèmes de liens** [lien google pour mieux comprendre](https://support.google.com/webmasters/answer/66356?hl=fr)
- Créer des pages avec **peu ou pas de contenu original**
- Le *[cloaking](https://support.google.com/webmasters/answer/66355?hl=fr)* , c'est-à-dire montrer à l'utilisateur **un contenu différent de celui que l'on présente au moteur de recherche**
- Les **textes et liens cachés**
- Les **Pages satellites** [*Doorway pages*](https://support.google.com/webmasters/answer/2721311?hl=fr)


**BING**

Les **principes de base** :

- **Produire du contenu clai**r, précis, facile à trouver, stimulant
- Des **titres clairs** et pertinents
- Bing récompense les liens qui ont grandi organiquement (pas compris)
- L**'influence sociale**
- **Vitesse** de chargement des pages
- Description des images avec l'**attribut alt**

**Ce qu'il faut éviter** :

- Le contenu mince, les pages avec principalement de la pub, ou avec des liens qui redirigent vers des sites mal rankés.
- Le système de liens (voir google, plus haut)
- Des URLS non précis, non clair, trop longs, non *keyword-inclusive*. Eviter aussi les caractères autres que les lettres.
- Le contenu dupliqué
- [L'accumulation de mots-clés](https://support.google.com/webmasters/answer/66358?hl=fr) : le *keyword-stuffing*
- Le *cloaking*

Lignes directrices pour représenter son business local sur Google
-------------------------

Pour faire partie du listing sur *Google my Business*, si l'on est une petite entreprise, il faut suivre certaines conventions. (peut s'appliquer au garage solidaire)

Les principes de base : 

- S'assurer d'être éligible au service; avoir une adresse physique ; produire des services en face-à-face
- Représenter précisément tous les aspects de l'entreprise/magasin : nom, adresse, téléphone, site web...

Ce qu'il faut éviter:

- Créer un listing Google my Business pour une entité **non éligibl**e.
- **Malreprésenter** le champ d'activité de l'entreprise
- **Utiliser une boite aux lettres, un bureau virtuel** plutôt qu'une vraie adresse postale.
- **S'attribuer de faux commentaires** positifs, ou en attribuer des négatifs à ses compétiteurs. 


Contenter les intentions de l'utilisateur (*user intent*)
------------------------------------------------------

L'intention de l'utilisateur, c'est **ce qu'il s'attend à trouver lorsqu'il fait une recherche** une internet.

Les** types communs d'*user intent*** :

- **Informatif** : Chercher une information. Ex: "Où se trouve le Guatemala ?"
- **Navigationnel** : Chercher un site précis. Ex: "Trivago"
- **Transactionnel** : Chercher à acheter quelque chose. Ex: "Vacances au Guatemala pas cher"

-> Entrer dans google des mots-clés permet de deviner les intentions de l'utilisateur vis-à-vis de ceux-ci et d'apprécier le SERP courant.

-> Analyser le contenu produits par ses  compétiteurs pour ce qu'ils offrent et que vous non.

Connaitre les buts du site
--------------------------

**KPI : Key Perfomance Indicators**. 

Exemple de KPI courants :

- Ventes
- Téléchargements
- Enregistrement d'emails
- Soumissions de formulaires de contact.
- Appels téléphoniques

Et si le site fait partie du listing Google my Business:

- Clicks-to-call
- Clicks-to-website
- Clicks-for-driving-directions

Le trafic et le ranking doivent être envisagés comme un moyen, pas comme une fin. 


Le fonctionnement des moteurs de recherche : Crawling, Indexing & Ranking
=========================================================================

Comment fonctionnent les moteurs de recherche ?
---------------------------

Les moteurs de recherche ont **trois fonctions primaires** :

- **Crawl** : càd **parcourir le net pour du contenu**, en analysant le contenu des URL/sites
- **Index** : Emmagasiner et organiser le contenu trouvé durant la phase de *crawl*
- **Rank** : Fournir les bouts de contenus les plus aptes à répondre à la requête de l'utilisateur.

Crawling & Index
---------------

Le processus de découverte durant lequel **les moteurs de recherche envoient des robots** (*aka* *crawlers* ou *spiders*) **pour trouver du contenu nouveau et mise à jour**. Le contenu est découvert par le biais de liens. 

Les *googlebots* commencent par chercher quelques pages puis **suivent les liens présents sur ces pages** pour trouver de nouvelles URLS.

**Caffeine** est le nom de l'index qui contient toute la base de données des URL trouvées. 

Pour que son contenu puisse être trouvé par ceux qui font des recherches, il faut donc **d'abord qu'il soit accessible aux *crawlers* et qu'il soit indexable**.

Phase de crawling : rendre les pages visibles
------------------------------------

Pour savoir combien de pages de votre site sont indexés il suffit de taper la requête suivante dans google :

    site:votresite.com

Pour avoir des résultats plus précis, il faut de la [Google Search Console](https://support.google.com/webmasters/answer/9128668?hl=fr&visit_id=637313705286187566-1853452635&rd=1)

Plusieurs raisons peuvent expliquer une absence de résultat à l'indexation:

- Un site trop récent
- Un site non lié à des sites externes
- La navigation du site trop compliquée pour les *crawlers*
- La présence de code qui bloque les moteurs de recherches : les *crawler directives*
- Un site pénalisé par Google pour des tactiques de spam

Robots.txt
--------

Les fichiers **Robots.txt sont localisés dans le répertoire racine** des sites et **contiennent des indications sur quelles parties du site ne devraient pas être rendues accessibles aux crawlers**. 

Quand Googlebot trouve un fichier Robots.txt, il suit les suggestions et commence à crawler le site. Il est utile de garder son contenu non-important dans le fichier Robots.txt pour optimiser la phase de crawling de son site.

**Attention**, certaines personnes mal intentionnés **cherchent les robots.txt pour y trouver des informations sensibles. **Il est donc **déconseillé d'y placer ses pages de login et d'administration**.

Comment faire pour que les crawlers puissent trouver le contenu le plus important. 
-------------------------------------

Si du contenu est protégé par connexion, alors le crawler ne peut pas y accéder. 

Les robots ne peuvent pas non plus accéder aux formulaires de recherche.

Le texte contenu dans des images, vidéos, gifs, n'est pas visible pour les crawlers. 

Si une page n'est pas liée au reste du reste, alors elle est invisible. 

Exemples d'erreurs de navigation typiques :

- Avoir une navigation mobile et desktop qui produisent des résultats différents.
- Les navigations où les *menu items* sont en JavaScript et pas en HTML.
- Des navigations spécifiques à chaque utilisateur.

Créer un [sitemap](https://support.google.com/webmasters/answer/183668?hl=fr) que l'on envoie ensuite à **GSC** est une bonne façon de s'assurer que Google trouve le contenu le plus important. 

Une *sitemap* est une liste d'URLs présents sur le site qui sont** utilisables par les crawlers pour découvrir et indexer le contenu**.

Un crawler peut rencontrer des erreurs lorsqu'il analyse le site. La partie Crawl Erros de la **GSC** peut fournir des indications sur l'erreur, ainsi que les fichiers de log du serveur.

Il faut savoir distinguer les **erreurs 400 qui sont des erreurs clients**, ce qui veut dire un problème de syntaxe de l'URL où qu'elle ne pointe vers rien (404), et **les erreurs 500 qui sont des erreurs de serveurs**. 

Le **code 301 de redirection** qui permet de suppléer à certaines de ces erreurs, en redirigeant d'une page non fonctionnelle à une qui l'est, **ne doit être utilisé que dans certains cas**. Exemple : il ne faut pas rediriger une ancienne URL vers une nouvelle qui ne *matche* pas avec le contenu de l'ancienne. 

Comment se rendre indexable ?
---------------------------

**Qu'un site puisse être trouvé ne siginifie pas qu'il puisse être indexé.** Ce sont deux procédés différents. 

Il est possible de voir la dernière fois que GoogleBot a crawlé un site en allant voir dans la version cache de la page. 

Les pages internet peuvent *être désindéxées. 

Quelques raisons pourquoi :

- Erreur 404
- Ajout d'un metatag noindex afin d'être omis de l'indexation
- Violation des principes d'indexation
- Bloquage du crawl

Dire aux moteurs de recherche comment être indexé 
-------------------------------------------------

Les **Robots meta directives**

Ce sont des instructions que l'on donne aux moteurs de recherche pour leur dire comment la page doit être traitée. 

Les **Robots meta tags**

Ils peuvent être utilisés au sein de la balise `<head>`. Exemples de meta tag :

- index/no index : indique si la page doit être crawlée ou non. No index indique aux crawlers que la page ne doit pas être indéxée.
- follow/nofollow : indique aux moteurs de recherche s'ils doivent suivre ou non les liens. 
- noarchive : pour empêcher les moteurs de recherche de conserver un cache de la page. 

Le **X-Robots tag**

Il est utilisé au sein de l'header http de l'URL et **offre plus de flexibilité et de fonctionnalitées** que les meta tags car il permet entre autres, d'utiliser les RegEx. 

Le fonctionnement du Ranking
---------------------------

Les moteurs de recherche **utilisent des algorithmes pour déterminer de la pertinence** du contenu indexé. 

Si le SEO a beaucoup évolué par rapport à ce qu'il était auparavant, c'est parce que les algorithmes de recherche se sont perfectionnés et sont devenus plus difficiles à tromper. 

Deux types de liens :

- Backlink/indbound link : les liens d'autres sites qui pointent vers notre page
- Interal link : liens présents sur des pages de notre site et qui renvoient à d'autres pages du site.

Le **PageRank**, qui fait partie de l'algorithme google, analyse les liens et **estime l'importance d'une page en fonction de la qualité et de la quantité des liens** qui pointent vers elle.

**Plus il y a de *backlinks*** venant de sites de confiance,** plus la page apparaitra haut** dans les résultats. 

